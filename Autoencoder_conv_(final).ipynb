{"cells":[{"cell_type":"markdown","metadata":{"id":"0HgP-Fk7Ikj1"},"source":["# Práctica 2\n","Miriam Méndez y Laura Lasso\n","## 1 Importación de librerías y carga de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTJKxmrHXyiO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","from itertools import product"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_Wk2nXYXyiQ"},"outputs":[],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5), (0.5))\n","    ])\n","\n","transform = transforms.ToTensor()\n","\n","mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n","                                          batch_size=64,\n","                                          shuffle=True)\n","\n","mnist_data_valid = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","data_loader_valid = torch.utils.data.DataLoader(dataset=mnist_data_valid,\n","                                          batch_size=64,\n","                                          shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgHZzwJWXyiR"},"outputs":[],"source":["dataiter = iter(data_loader)\n","images, labels = next(dataiter)\n","print(torch.min(images), torch.max(images))"]},{"cell_type":"markdown","metadata":{"id":"ZYIZa7HcI1kR"},"source":["## 2 CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MU_RFGQXyiS"},"outputs":[],"source":["class Autoencoder_Conv(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder_Conv, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # Cambio a capas convolucionales\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 7)  # No hay padding aquí\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, 7),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n","            nn.Sigmoid()  # Usar Sigmoid al final si las imágenes están entre 0 y 1\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23TOba33L4qf"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jc81PNJVXyiS"},"outputs":[],"source":["class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # N, 1, 28, 28\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 16, 3, stride=2, padding=1), # -> N, 16, 14, 14\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> N, 32, 7, 7\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 7) # -> N, 64, 1, 1\n","        )\n","\n","        # N , 64, 1, 1\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, 7), # -> N, 32, 7, 7\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # N, 16, 14, 14 (N,16,13,13 without output_padding)\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # N, 1, 28, 28  (N,1,27,27)\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","\n","# Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n","# Input [-1, +1] -> use nn.Tanh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZblZOXYXyiS"},"outputs":[],"source":["# model = Autoencoder()\n","model = Autoencoder_Conv()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.AdamW(model.parameters(),\n","                             lr=1e-3,\n","                             weight_decay=1e-5)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSBLUEawXyiS"},"outputs":[],"source":["num_epochs = 10\n","outputs = []\n","\n","for epoch in range(num_epochs):\n","    for (img, _) in data_loader:\n","        img = img.to(device)\n","\n","        # Si img no tiene el canal, agrega una dimensión de canal\n","        if len(img.size()) == 3:\n","            img = img.unsqueeze(1)\n","\n","        recon = model(img)  # Pasa la imagen a través del modelo\n","        loss = criterion(recon, img)  # Calcula la pérdida\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n","    outputs.append((epoch, img, recon))\n","\n","# Guardar el modelo entrenado\n","torch.save(model.state_dict(), 'autoencoder_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2lI7e8cXyiS"},"outputs":[],"source":["for k in range(0, num_epochs, 4):\n","    plt.figure(figsize=(9, 2))\n","    plt.gray()\n","    imgs = outputs[k][1].detach().cpu().numpy()\n","    recon = outputs[k][2].detach().cpu().numpy()\n","    for i, item in enumerate(imgs):\n","        if i >= 9: break\n","        plt.subplot(2, 9, i+1)\n","        item = item.reshape(-1, 28, 28)\n","        plt.imshow(item[0])\n","\n","    for i, item in enumerate(recon):\n","        if i >= 9: break\n","        plt.subplot(2, 9, 9+i+1)  # row_length + i + 1\n","        item = item.reshape(-1, 28, 28)\n","        plt.imshow(item[0])\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"jKb2Oux9XyiT"},"source":["### 2.3 Pruebas sobre el conjunto de validación"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjqaVRWxXyiU"},"outputs":[],"source":["model.eval()  # Asegúrate de que el modelo esté en modo de evaluación\n","\n","dataiter = iter(data_loader_valid)\n","images, labels = next(dataiter)\n","\n","plt.figure(figsize=(9, 2))\n","plt.gray()\n","\n","for i, img in enumerate(images[:9]):\n","    with torch.no_grad():\n","        img = img.unsqueeze(0).to(device)  # Asegúrate de que la imagen tenga una dimensión de canal y esté en el dispositivo correcto\n","        # No necesitas aplanar la imagen: img2 = img.view(-1, 28*28)\n","        pred = model(img)  # Obtener la predicción del modelo\n","        recon = pred[0].cpu().detach().numpy()  # Mover el tensor de vuelta a la CPU para la conversión a NumPy\n","        recon2 = recon.reshape(28, 28)  # Cambiar la forma para visualización\n","\n","        plt.subplot(2, 9, i+1)\n","        plt.imshow(img.cpu().squeeze(), cmap='gray')  # Muestra la imagen original, asegurándote de eliminar cualquier dimensión unitaria\n","        plt.axis('off')  # Ocultar los ejes\n","\n","        plt.subplot(2, 9, i+1+9)\n","        plt.imshow(recon2, cmap='gray')  # Visualizar la reconstrucción\n","        plt.axis('off')  # Ocultar los ejes\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ww28xkvsXyiU"},"source":["## 3 Ejercicios\n","### 3.1 Primera parte\n","\n","- Crea un supresor de ruido de imágenes. Añade ruido a las imágenes de entrada y compáralas a la salida con las originales. Varía los hiperparámetros y busca el mejor supresor.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3IpYXtqXyiU"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","from torch import nn\n","from torch.utils.data import DataLoader\n","\n","# Función para agregar ruido a las imágenes\n","def add_noise(img):  # Add noise to the image\n","    noise = torch.randn(img.size()).to(device) * 0.2\n","    noisy_img = img + noise\n","    return noisy_img\n","\n","# Definición de los hiperparámetros para la búsqueda en cuadrícula\n","tasas_de_aprendizaje = [1e-3, 5e-4, 1e-4]\n","decays_de_peso = [1e-5, 1e-4, 1e-3]\n","\n","# Variables para almacenar los mejores hiperparámetros\n","mejor_perdida = float('inf')\n","mejores_hiperparametros = {}\n","\n","# Ciclo de búsqueda en cuadrícula\n","for lr in tasas_de_aprendizaje:\n","    for wd in decays_de_peso:\n","        model = Autoencoder_Conv().to(device)\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","        criterion = nn.MSELoss()\n","\n","        # Ciclo de entrenamiento\n","        num_epochs = 10\n","        for epoch in range(num_epochs):\n","            for img, _ in data_loader:\n","                img = img.to(device)  # Mover el tensor al dispositivo correcto\n","                optimizer.zero_grad()\n","                noisy_imgs = add_noise(img)\n","                recon = model(noisy_imgs)\n","                loss = criterion(recon, img)\n","                loss.backward()\n","                optimizer.step()\n","\n","            print(f'Epoch:{epoch+1}, LR: {lr}, Weight Decay: {wd}, Loss:{loss.item():.4f}')\n","\n","         # Evaluación del modelo\n","        model.eval()\n","        perdida_validacion = 0\n","        total_samples = 0\n","        with torch.no_grad():\n","            for (img, _) in data_loader_valid:\n","                img = img.to(device)\n","                noisy_imgs = add_noise(img)\n","                noisy_imgs_flat = noisy_imgs.view(-1, 28*28)\n","                recon = model(noisy_imgs)\n","                perdida_validacion += criterion(recon, img).item() * img.size(0)  # Usa la imagen original para calcular la pérdida\n","                total_samples += img.size(0)\n","\n","        perdida_validacion /= total_samples\n","\n","        # Actualización de los mejores hiperparámetros si se encontró un mejor modelo\n","        if perdida_validacion < mejor_perdida:\n","            mejor_perdida = perdida_validacion\n","            mejores_hiperparametros = {'tasa_de_aprendizaje': lr, 'decay_de_peso': wd}\n","\n","# Imprimir los mejores hiperparámetros\n","print(f\"Mejores hiperparámetros encontrados: Tasa de aprendizaje = {mejores_hiperparametros['tasa_de_aprendizaje']}, \"\n","      f\"Decay de peso = {mejores_hiperparametros['decay_de_peso']}. \"\n","      f\"Pérdida de validación = {mejor_perdida}\")\n","\n","# Visualización de las imágenes (usando los mejores hiperparámetros encontrados)\n","# Visualización de las imágenes (usando los mejores hiperparámetros encontrados)\n","fig, axes = plt.subplots(nrows=3, ncols=10, figsize=(20, 6))\n","with torch.no_grad():\n","    for i in range(10):\n","        # Imágenes originales\n","        img_orig = img[i].cpu().squeeze()  # img es una imagen del data_loader_valid\n","        axes[0, i].imshow(img_orig, cmap='gray')\n","        axes[0, i].axis('off')\n","\n","        # Imágenes con ruido\n","        noisy_img = add_noise(img[i].unsqueeze(0).to(device))  # Añadir ruido, asegurarse de tener dimensión de canal\n","        axes[1, i].imshow(noisy_img.cpu().squeeze(), cmap='gray')\n","        axes[1, i].axis('off')\n","\n","        # Imágenes reconstruidas\n","        recon_img = model(noisy_img).cpu().squeeze()  # Pasar la imagen con ruido directamente al modelo\n","        axes[2, i].imshow(recon_img, cmap='gray')\n","        axes[2, i].axis('off')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"oKSyDaAkuW5n"},"source":["### 3.2 Parte 2: Hacer super resolución\n","#### 3.2.1 Definición de las funciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UZs49Js2ioP"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision\n","\n","def train_model(model, criterion, optimizer, dim, num_epochs, train_loader):\n","        for epoch in range(num_epochs):\n","            for data in train_loader:\n","                img, _ = data\n","                optimizer.zero_grad()\n","                output = model(img)\n","\n","                # Ajustar las dimensiones de la imagen de salida para que coincida con la entrada\n","                output = nn.functional.interpolate(output, size=(dim, dim), mode='bilinear', align_corners=False)\n","\n","                loss = criterion(output, img)\n","                loss.backward()\n","                optimizer.step()\n","\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","def download_data(dim, batch_size=64):\n","        transform = transforms.Compose([\n","        transforms.Resize((dim, dim)),  # Ajustar el tamaño de las imágenes de entrada a 14x14\n","        transforms.ToTensor()])\n","\n","        # Descargar el conjunto de datos MNIST\n","        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","        # Crear DataLoader para facilitar el manejo de los datos\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        return train_dataset, train_loader\n","\n","# Visualizar las imágenes originales y reconstruidas\n","def imshow(img, title):\n","        img = img.numpy().transpose((1, 2, 0))\n","        plt.imshow(img, cmap='gray')\n","        plt.title(title)\n","        plt.show()\n","\n","def visualize_results(model, train_dataset):\n","        # Establecer el modelo en modo de evaluación\n","        model.eval()\n","\n","        # Obtener un lote de datos de prueba\n","        test_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n","        test_batch, _ = next(iter(test_loader))\n","\n","        # Realizar la inferencia en el lote de prueba\n","        with torch.no_grad():\n","          reconstructed_batch = model(test_batch)\n","        # Visualizar imágenes originales\n","        imshow(torchvision.utils.make_grid(test_batch), \"Original Images\")\n","\n","        # Visualizar imágenes reconstruidas\n","        imshow(torchvision.utils.make_grid(reconstructed_batch), \"Reconstructed Images\")"]},{"cell_type":"markdown","metadata":{"id":"_6qo5IHvXwzo"},"source":["#### 3.2.2 Entrada 14x14 y salida 28x28\n","##### 3.2.2.1 Definición de la CNN, entrenamiento y visualización de resultados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RiqD5Xq1lo_6"},"outputs":[],"source":["# Definir el modelo con encoder y decoder\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","            #nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rS9QaearXu7O"},"outputs":[],"source":["dim = 14\n","train_dataset, train_loader = download_data(dim)\n","\n","# Instanciar el modelo\n","autoencoder = Autoencoder()\n","\n","# Definir la función de pérdida y el optimizador\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n","num_epochs = 10\n","\n","# Entrenar el modelo\n","train_model(autoencoder, criterion, optimizer, dim, num_epochs, train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-JdNNorX5QY"},"outputs":[],"source":["import torchvision\n","visualize_results(autoencoder, train_dataset)"]},{"cell_type":"markdown","metadata":{"id":"sUROTcUOhNii"},"source":["##### 3.2.2.2 Definición de la CNN mejorada, entrenamiento y visualización de resultados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xl4mBwUVmIXt"},"outputs":[],"source":["# Definir el modelo de superresolución adaptado\n","class SuperResolutionModel(nn.Module):\n","    def __init__(self):\n","        super(SuperResolutionModel, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1),  # Ajuste el stride para obtener una salida de tamaño 28x28\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # Upsample para obtener una salida de tamaño 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlG25Dr0hLHg"},"outputs":[],"source":["# Entrenar el modelo\n","dim = 14\n","train_dataset, train_loader = download_data(dim)\n","\n","# Instanciar el modelo\n","super_resolution_model = SuperResolutionModel()\n","\n","# Definir la función de pérdida y el optimizador\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(super_resolution_model.parameters(), lr=0.001)\n","num_epochs = 5\n","\n","# Entrenar el modelo\n","train_model(super_resolution_model, criterion, optimizer, dim, num_epochs, train_loader)"]},{"cell_type":"code","source":["visualize_results(super_resolution_model, train_dataset)"],"metadata":{"id":"Tt1TNtE4wggw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"izz-v29BYCaK"},"source":["\n","#### 3.2.3 Entrada 28x28 y salida 56x56\n","##### 3.2.3.1 Definición de la CNN, entrenamiento y visualización de resultados"]},{"cell_type":"code","source":["# Definir el modelo con encoder y decoder\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","            #nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"6pSaCYwhwNEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenar el modelo\n","dim = 28\n","train_dataset, train_loader = download_data(dim)\n","\n","# Instanciar el modelo\n","autoencoder = Autoencoder()\n","\n","# Definir la función de pérdida y el optimizador\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n","num_epochs = 10\n","\n","# Entrenar el modelo\n","train_model(autoencoder, criterion, optimizer, dim, num_epochs, train_loader)"],"metadata":{"id":"eQrfO8frwPTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnE2wWkL2_47"},"outputs":[],"source":["visualize_results(autoencoder, train_dataset)"]},{"cell_type":"markdown","metadata":{"id":"7nQp5IoaYLfl"},"source":["\n","##### 3.2.3.2 Definición de la CNN mejorada, entrenamiento y visualización de resultados"]},{"cell_type":"code","source":["# Definir el modelo de superresolución adaptado\n","class SuperResolutionModel(nn.Module):\n","    def __init__(self):\n","        super(SuperResolutionModel, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1),  # Ajuste el stride para obtener una salida más grande\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # Upsample para obtener una salida de tamaño 56x56\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"cj9X92TrwRRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenar el modelo\n","dim = 28\n","train_dataset, train_loader = download_data(dim)\n","\n","# Instanciar el modelo\n","super_resolution_model = SuperResolutionModel()\n","\n","# Definir la función de pérdida y el optimizador\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(super_resolution_model.parameters(), lr=0.001)\n","num_epochs = 2\n","\n","# Entrenar el modelo\n","train_model(super_resolution_model, criterion, optimizer, dim, num_epochs, train_loader)"],"metadata":{"id":"cny2_b0WwS-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_results(super_resolution_model, train_dataset)"],"metadata":{"id":"3-7YzV2VwUbu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oieZ3FAfsYpS"},"source":["#### 3.2.4 Entrada 7x7 y salida 28*28"]},{"cell_type":"code","source":["class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=1),  # Cambiado el tamaño del kernel a 2\n","            nn.ReLU(),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, padding=1, output_padding=1),  # Cambiado el tamaño del kernel a 2\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"01KVU3a8qXg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dim = 7  # Cambiado a 7 para trabajar con imágenes de 7x7\n","train_dataset, train_loader = download_data(dim)\n","\n","# Instanciar el modelo\n","autoencoder = Autoencoder()\n","\n","# Definir la función de pérdida y el optimizador\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n","num_epochs = 10\n","\n","# Entrenar el modelo\n","train_model(autoencoder, criterion, optimizer, dim, num_epochs, train_loader)\n"],"metadata":{"id":"0OfpQs-gqhag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_results(autoencoder, train_dataset)"],"metadata":{"id":"zfqx04Wvqvf5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 3.2.4.1 Definición de la CNN mejorada, entrenamiento y visualización de resultados"],"metadata":{"id":"Or5CpiSjo24h"}},{"cell_type":"code","source":["# Definir el modelo de superresolución adaptado\n","class SuperResolutionModel(nn.Module):\n","    def __init__(self):\n","        super(SuperResolutionModel, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=2, padding=1, stride=2),  # Cambiado el tamaño del kernel a 2\n","            nn.ReLU(inplace=True)\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=1),  # Cambiado el tamaño del kernel a 2\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False),  # Upsample para obtener una salida de tamaño 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"am5HRI7qqWFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenar el modelo\n","dim = 7  # Cambiado a 7 para trabajar con imágenes de 7x7\n","train_dataset, train_loader = download_data(dim)\n","\n","# Instanciar el modelo\n","super_resolution_model = SuperResolutionModel()\n","\n","# Definir la función de pérdida y el optimizador\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(super_resolution_model.parameters(), lr=0.001)\n","num_epochs = 5\n","\n","# Entrenar el modelo\n","train_model(super_resolution_model, criterion, optimizer, dim, num_epochs, train_loader)"],"metadata":{"id":"UPf_i5b7rYHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_results(super_resolution_model, train_dataset)"],"metadata":{"id":"GtM7b_2jrd13"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}